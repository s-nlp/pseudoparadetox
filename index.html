<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="PseudoParaDetox: Generating High-Quality Detoxification Data with Abliterated LLMs">
  <meta property="og:title" content="PseudoParaDetox: Abliterated LLMs for Text Detoxification"/>
  <meta property="og:description" content="Exploring the potential of modern open source LLMs to annotate parallel data for text detoxification"/>
  <meta property="og:url" content="https://s-nlp.github.io/pseudoparadetox/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="PseudoParaDetox: Abliterated LLMs for Text Detoxification">
  <meta name="twitter:description" content="PseudoParaDetox: Abliterated LLMs for Text Detoxification">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLMs Detoxification Text Style Transfer">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PseudoParaDetox: Abliterated LLMs for Text Detoxification</title>
  <link rel="icon" type="image/x-icon" href="static/images/sk.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto:Daniil.Moskovskiy@skoltech.ru" target="_blank">Daniil Moskovskiy</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="mailto:Sergey.Pletenev@skoltech.ru" target="_blank">Sergey Pletenev</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="mailto:A.Panchenko@skoltech.ru" target="_blank">Alexander Panchenko</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Skoltech, AIRI, HSE<br>EMNLP Findings 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://drive.google.com/file/d/1SaEVM8Z6I2T7vULlWFQ9YbKCe9Kd1lxt/view?usp=sharing" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/s-nlp/pseudoparadetox" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://drive.google.com/file/d/1SaEVM8Z6I2T7vULlWFQ9YbKCe9Kd1lxt/view?usp=sharing" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The lack of high-quality training data remains a significant challenge in NLP. Manual annotation methods, such as crowdsourcing, are costly, require intricate task design skills, and, if used incorrectly, may result in poor data quality. From the other hand, LLMs have demonstrated proficiency in many NLP tasks, including zero-shot and few-shot data annotation. However, they often struggle with text detoxification due to alignment constraints and fail to generate the required detoxified text. This work explores the potential of modern open source LLMs to annotate parallel data for text detoxification. Using the recent technique of activation patching, we generate a pseudo-parallel detoxification dataset based on ParaDetox. The detoxification model trained on our generated data shows comparable performance to the original dataset in automatic detoxification evaluation metrics and superior quality in manual evaluation and side-by-side comparisons.          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            This work explores the potential of modern open source LLMs to annotate parallel data for text detoxification. To achieve this, we use the activation patching technique to generate a pseudo-parallel detoxification dataset based on ParaDetox. The methodology involves the following steps:
          </p>
          <ol>
            <li><strong>Data Collection:</strong> We start by collecting the ParaDetox dataset, which contains parallel data of toxic and detoxified sentences.</li>
            <li><strong>LLM Annotation:</strong> We use open source LLMs to generate detoxified sentences for the toxic sentences in the ParaDetox dataset.</li>
            <li><strong>Activation Patching:</strong> We apply the activation patching technique to refine the generated detoxified sentences and improve their quality. This involves calculating the difference vector in activations for each layer of the model using the following formula:
              <p>
                \[
                r_{l} = a^{\text{harmful}}_{l} - a^{\text{harmless}}_{l}
                \]
              </p>
              where \( a^{\text{harmful}}_{l} \) and \( a^{\text{harmless}}_{l} \) are the average of the residual stream activations at the last token position for each layer \( l \) of the model for harmful and harmless instructions, respectively. We then normalize the difference vectors and select the "best" refusal stream direction \( \hat{r}_{\text{best}} \) by evaluating \( \hat{r}_i \) on a separate set of harmful instructions. Finally, we modify the weight matrices of the model directly using the following formula:
              <p>
                \[
                \tilde{W}_{\text{out}} = W_{\text{out}} - \hat{r}_{\text{best}}\hat{r}_{\text{best}}^{\operatorname{T}}W_{\text{out}}.
                \]
              </p>
            </li>
            <li><strong>Model Training:</strong> We train a detoxification model, such as BART, on the generated pseudo-parallel detoxification dataset.</li>
            <li><strong>Automatic Metrics</strong> In detoxification evaluation, we follow the pipeline presented in ParaDetox. We calculate style transfer accuracy (<strong>STA</strong>), similarity (<strong>SIM</strong>), fluency (<strong>FL</strong>), and their sentence-level average - Joint score (<strong>J</strong>):</p>

            <p>
              <span>\[
                \textbf{J}(x_i, y_i) = \frac{1}{n} \sum_{i=1}^{n} \textbf{STA}(x_i) \textbf{SIM}(x_i, y_i) \textbf{FL}(x_i).
              \]</span>
            </p>          </ol></li>
        </div>
      </div>
    </div>
  </div>
</section>



<style>
  .image-container-2 {
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    width: 900px;
    margin: 0 auto;
  }

  .image-container-2 img {
    width: 100%;
    height: auto;
  }

  .image-container-2 h2 {
    margin-top: 10px;
    width: 100%;
  }
</style>

<section class="hero is-light">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Results</h2> <!-- Corrected the <Results> tag -->
    <div class="container">
      <div class="image-row">  
        <div class="image-container-2">
          <img src="static/images/results_automatic.png" alt="PseudoParadetox: Results of Automatic Evaluation.">
          <h2 class="subtitle has-text-centered">
            Results of automatic detoxification evaluation after training BART on the original ParaDetox data (highlighted in <span style="background-color:lightgray;">gray</span>) and generated with LLMs PseudoParaDetox data in 0-shot and 10-shot settings. A.P. stands for Activation Patched models, <span style="text-decoration:line-through;">X</span> stands for models used <em>as is</em>. Best results for each setting (0-shot/10-shot) are <strong>bold</strong>, and the best overall results are <strong><u>underlined bold</u></strong>.
          </h2>
        </div>
      </div>
      <div class="image-row">  
        <div class="image-container-2">
          <img src="static/images/results_manual.png" alt="PseudoParadetox: Results of Manual Evaluation.">
          <h2 class="subtitle has-text-centered">
            Results of manual detoxification evaluation after training BART on the original ParaDetox data (highlighted in <span style="background-color:lightgray;">gray</span>) and generated with LLMs PseudoParaDetox data in 0-shot and 10-shot settings. A.P. stands for Activation Patched models, <span style="text-decoration:line-through;">X</span> stands for models used <em>as is</em>. Best results for each setting (0-shot/10-shot) are <strong>bold</strong>, and the best overall results are <strong><u>underlined bold</u></strong>.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<style>
  .image-row {
    display: flex;
    justify-content: space-between;
  }

  .image-container {
    flex: 1;
    margin: 0 10px;
    text-align: center;
    width: 100%;
  }

  .image-container img {
    width: 100%;
    height: auto;
    max-height: 500px; /* Adjust the max-height as needed */
  }
</style>

<section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Side-by-Side Evaluations</h2> <!-- Corrected the <Results> tag -->
    <div class="container">
      <div class="image-row">
        <!-- <div class="image-container">
          <img src="static/images/carousel1.jpg" alt="First Image Description">
          <h2 class="subtitle has-text-centered">First image description.</h2>
        </div> -->
        <div class="image-container">
          <img src="static/images/PseudoParaDetox_llm_page.jpg" alt="Second Image Description">
          <h2 class="subtitle has-text-centered">
            Side-by-side evaluation BART trained on ParaDetox versus PseudoParaDetox (generated by activation patched LLMs) on a held-out test set.
            Win of LLM-generated is highlighted with <span style="background-color:teal; color:white;">teal</span>,
            Tie is highlighted with <span style="background-color:beige;">beige</span>,
            and ParaDetox is highlighted with <span style="background-color:lightgrey;">grey</span>.
          </h2>
        </div>
        <div class="image-container">
          <img src="static/images/PseudoParaDetox_llm_page.jpg" alt="Second Image Description">
          <h2 class="subtitle has-text-centered">
            Side-by-side evaluation BART trained on ParaDetox versus PseudoParaDetox (generated by activation patched LLMs) on a held-out test set.
            Win of LLM-generated is highlighted with <span style="background-color:teal; color:white;">teal</span>,
            Tie is highlighted with <span style="background-color:beige;">beige</span>,
            and ParaDetox is highlighted with <span style="background-color:lightgrey;">grey</span>.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<style>
  .image-container-3 {
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
  }

  .image-container-3 img {
    width: 900px;
    height: 900px;
  }

  .image-container-3 h2 {
    margin-top: 10px;
  }
</style>


<section class="hero is-light">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Text Output Examples</h2> <!-- Corrected the <Results> tag -->
    <div class="container">
      <div class="image-row">  
        <div class="image-container-3">
          <img src="static/images/text_examples_8b.png" alt="Second Image Description">
          <h2 class="subtitle has-text-centered">
            Examples of text detoxification on a private test set of ParaDetox for 8B Llama 3 models. Original toxic sentence is highlighted with <span style="background-color:lightpink;">pink</span>, Human Reference detoxification is highlighted with <span style="background-color:lightgreen;">green</span>.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <div class="image-row">  
        <div class="image-container-3">
          <img src="static/images/text_examples_70b.png" alt="Second Image Description">
          <h2 class="subtitle has-text-centered">
            Examples of text detoxification on a private test set of ParaDetox for 70B Llama 3 models. Original toxic sentence is highlighted with <span style="background-color:lightpink;">pink</span>, Human Reference detoxification is highlighted with <span style="background-color:lightgreen;">green</span>.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
