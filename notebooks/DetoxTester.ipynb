{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    NllbTokenizerFast,\n",
    "    BartTokenizerFast,\n",
    "    T5TokenizerFast,\n",
    "    BartForConditionalGeneration,\n",
    "    T5ForConditionalGeneration,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "\n",
    "def detoxify_batch(\n",
    "    texts: List[str],\n",
    "    model: Union[BartForConditionalGeneration, T5ForConditionalGeneration],\n",
    "    tokenizer: PreTrainedTokenizerFast,\n",
    "    batch_size: int = 32,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Detoxify a batch of texts.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): The list of texts to detoxify.\n",
    "        model (Union[BartForConditionalGeneration, T5ForConditionalGeneration]): The detoxification model.\n",
    "        tokenizer (PreTrainedTokenizerFast): The tokenizer for the detoxification model.\n",
    "        batch_size (int, optional): The batch size for detoxification. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The detoxified texts.\n",
    "    \"\"\"\n",
    "    detoxified = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Detoxifying\"):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_detoxified = model.generate(\n",
    "            **tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "            ,max_new_tokens=60, do_sample=False, temperature=None, top_p=None,\n",
    "        )\n",
    "        detoxified.extend(\n",
    "            tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "            for tokens in batch_detoxified\n",
    "        )\n",
    "    return detoxified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/test_toxic_parallel.txt\", \"r\") as f:\n",
    "    test_en = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "poc_path = \"/models/bart_synth/checkpoint-2780/\"\n",
    "orig_path =\"/home/models/bart_orig/checkpoint-2780/\"\n",
    "bart_paradetox = \"s-nlp/bart-base-detox\"\n",
    "\n",
    "for path, name in zip([poc_path, orig_path], [\"poc\", \"orig\"]):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(path).cuda().eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "    detoxed = detoxify_batch(test_en, model=model, tokenizer=tokenizer, batch_size=128)\n",
    "\n",
    "    with open(f\"../experiments/detoxed_{name}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(detoxed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/utils/evaluate.py \\\n",
    "    --source_list ../data/test_toxic_parallel.txt \\\n",
    "    --references_list ../data/test_neutral_parallel.txt \\\n",
    "    --input_path /results/generated/Meta-Llama-3-8B-Instruct-abliterated-v3.5_10shot_t08_p09.txt \\\n",
    "    --output_dir /results/generated.results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
